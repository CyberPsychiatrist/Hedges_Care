{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) and CNN Implementation\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Basic Exploratory Data Analysis (EDA) techniques\n",
    "2. Convolutional Neural Network (CNN) implementation\n",
    "3. Practical example with plant emissions data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Display versions for reproducibility\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Data Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset (using a sample approach for large files)\n",
    "file_path = 'Notebooks/Data/global_hybrid_plants_emissions_2000_2024.csv'\n",
    "\n",
    "# Read the first few rows to understand the structure\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(\"Dataset loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading file: {e}\")\n",
    "    # Create a sample dataset for demonstration\n",
    "    np.random.seed(42)\n",
    "    years = np.arange(2000, 2025)\n",
    "    plants = ['Plant_A', 'Plant_B', 'Plant_C', 'Plant_D', 'Plant_E']\n",
    "    \n",
    "    data = []\n",
    "    for year in years:\n",
    "        for plant in plants:\n",
    "            data.append({\n",
    "                'Year': year,\n",
    "                'Plant_Name': plant,\n",
    "                'CO2_Emissions': np.random.normal(1000, 200),\n",
    "                'Methane_Emissions': np.random.normal(50, 10),\n",
    "                'Nitrous_Oxide': np.random.normal(20, 5),\n",
    "                'Energy_Production': np.random.normal(500, 100),\n",
    "                'Efficiency_Rate': np.random.uniform(0.3, 0.9),\n",
    "                'Category': np.random.choice(['High', 'Medium', 'Low'])\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    print(\"Created sample dataset for demonstration\")\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"\\n=== Dataset Info ===\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(\"\\n=== First 5 rows ===\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Basic Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"=== Missing Values ===\")\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values[missing_values > 0])\n",
    "if missing_values.sum() == 0:\n",
    "    print(\"No missing values found!\")\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\n=== Descriptive Statistics ===\")\n",
    "display(df.describe())\n",
    "\n",
    "# Data types\n",
    "print(\"\\n=== Data Types ===\")\n",
    "display(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution analysis for numerical columns\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "print(f\"=== Numerical columns: {list(numerical_cols)} ===\")\n",
    "\n",
    "# Plot histograms for numerical features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, col in enumerate(numerical_cols):\n",
    "    if i < len(axes):\n",
    "        df[col].hist(bins=20, ax=axes[i], alpha=0.7)\n",
    "        axes[i].set_title(f'Distribution of {col}')\n",
    "        axes[i].set_xlabel(col)\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical data analysis\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "print(f\"=== Categorical columns: {list(categorical_cols)} ===\")\n",
    "\n",
    "# Plot categorical distributions\n",
    "if len(categorical_cols) > 0:\n",
    "    fig, axes = plt.subplots(1, min(len(categorical_cols), 3), figsize=(15, 5))\n",
    "    if len(categorical_cols) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, col in enumerate(categorical_cols[:3]):\n",
    "        df[col].value_counts().plot(kind='bar', ax=axes[i])\n",
    "        axes[i].set_title(f'Distribution of {col}')\n",
    "        axes[i].set_xlabel(col)\n",
    "        axes[i].set_ylabel('Count')\n",
    "        axes[i].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "print(\"=== Correlation Matrix ===\")\n",
    "correlation_matrix = df[numerical_cols].corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Matrix of Numerical Features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display strong correlations (|r| > 0.5)\n",
    "strong_correlations = correlation_matrix.unstack()\n",
    "strong_correlations = strong_correlations[abs(strong_correlations) > 0.5]\n",
    "strong_correlations = strong_correlations[strong_correlations != 1]\n",
    "if len(strong_correlations) > 0:\n",
    "    print(\"\\nStrong correlations found:\")\n",
    "    display(strong_correlations.drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series analysis if Year column exists\n",
    "if 'Year' in df.columns:\n",
    "    print(\"=== Time Series Analysis ===\")\n",
    "    \n",
    "    # Group by year and calculate means\n",
    "    yearly_data = df.groupby('Year').mean(numeric_only=True)\n",
    "    \n",
    "    # Plot trends over time\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    # Select a few key metrics to plot\n",
    "    metrics = ['CO2_Emissions', 'Energy_Production', 'Efficiency_Rate']\n",
    "    if 'Methane_Emissions' in yearly_data.columns:\n",
    "        metrics.append('Methane_Emissions')\n",
    "    \n",
    "    for i, metric in enumerate(metrics[:4]):\n",
    "        if metric in yearly_data.columns:\n",
    "            yearly_data[metric].plot(ax=axes[i], marker='o')\n",
    "            axes[i].set_title(f'{metric} Trend Over Time')\n",
    "            axes[i].set_xlabel('Year')\n",
    "            axes[i].set_ylabel(metric)\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Year-over-year percentage change\n",
    "    print(\"\\nYear-over-Year Percentage Change:\")\n",
    "    yoy_change = yearly_data.pct_change() * 100\n",
    "    display(yoy_change.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Data Preprocessing for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for CNN implementation\n",
    "print(\"=== Data Preprocessing for CNN ===\")\n",
    "\n",
    "# Create a classification target based on efficiency\n",
    "df['Efficiency_Category'] = pd.cut(\n",
    "    df['Efficiency_Rate'], \n",
    "    bins=[0, 0.33, 0.66, 1], \n",
    "    labels=['Low', 'Medium', 'High']\n",
    ")\n",
    "\n",
    "# Encode categorical variables\n",
    "df_encoded = df.copy()\n",
    "for col in categorical_cols:\n",
    "    if col != 'Efficiency_Category':\n",
    "        df_encoded[col] = pd.Categorical(df[col]).codes\n",
    "\n",
    "# Prepare features and target\n",
    "X = df_encoded.drop(['Efficiency_Category', 'Efficiency_Rate'], axis=1, errors='ignore')\n",
    "y = df_encoded['Efficiency_Category']\n",
    "\n",
    "# Handle missing values in X\n",
    "X = X.fillna(X.mean())\n",
    "\n",
    "# Encode target variable\n",
    "y_encoded = pd.Categorical(y).codes\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Unique classes: {np.unique(y)}\")\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\nTraining set size: {X_train_scaled.shape[0]} samples\")\n",
    "print(f\"Testing set size: {X_test_scaled.shape[0]} samples\")\n",
    "\n",
    "display(X_train_scaled[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: CNN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape data for CNN (treating features as a 1D image)\n",
    "n_features = X_train_scaled.shape[1]\n",
    "height = 1\n",
    "width = n_features\n",
    "channels = 1\n",
    "\n",
    "# Reshape the data\n",
    "X_train_cnn = X_train_scaled.reshape(-1, height, width, channels)\n",
    "X_test_cnn = X_test_scaled.reshape(-1, height, width, channels)\n",
    "\n",
    "print(f\"Reshaped training data: {X_train_cnn.shape}\")\n",
    "print(f\"Reshaped testing data: {X_test_cnn.shape}\")\n",
    "\n",
    "# Define CNN model\n",
    "def create_cnn_model(input_shape, num_classes):\n",
    "    model = keras.Sequential([\n",
    "        # Input layer\n",
    "        layers.Input(shape=input_shape),\n",
    "        \n",
    "        # First convolutional block\n",
    "        layers.Conv2D(32, (1, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((1, 2)),\n",
    "        \n",
    "        # Second convolutional block\n",
    "        layers.Conv2D(64, (1, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((1, 2)),\n",
    "        \n",
    "        # Flatten and dense layers\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "num_classes = len(np.unique(y_train))\n",
    "cnn_model = create_cnn_model((height, width, channels), num_classes)\n",
    "\n",
    "# Compile the model\n",
    "cnn_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Display model architecture\n",
    "print(\"=== CNN Model Architecture ===\")\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the CNN model\n",
    "print(\"=== Training CNN Model ===\")\n",
    "\n",
    "history = cnn_model.fit(\n",
    "    X_train_cnn, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=1,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "ax1.plot(history.history['accuracy'])\n",
    "ax1.plot(history.history['val_accuracy'])\n",
    "ax1.set_title('Model Accuracy')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "ax2.plot(history.history['loss'])\n",
    "ax2.plot(history.history['val_loss'])\n",
    "ax2.set_title('Model Loss')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "print(\"=== Model Evaluation ===\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred = cnn_model.predict(X_test_cnn)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "test_accuracy = np.mean(y_pred_classes == y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "class_names = ['Low', 'Medium', 'High']\n",
    "print(classification_report(\n",
    "    y_test, \n",
    "    y_pred_classes, \n",
    "    target_names=class_names\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "print(\"=== Confusion Matrix ===\")\n",
    "cm = confusion_matrix(y_test, y_pred_classes)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    cm, \n",
    "    annot=True, \n",
    "    fmt='d', \n",
    "    cmap='Blues',\n",
    "    xticklabels=class_names,\n",
    "    yticklabels=class_names\n",
    ")\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Feature importance analysis (simplified for CNN)\n",
    "print(\"=== Feature Analysis ===\")\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': np.abs(cnn_model.layers[-2].get_weights()[0]).mean(axis=1)\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=feature_importance.head(10), x='Importance', y='Feature')\n",
    "plt.title('Top 10 Feature Importances')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Advanced CNN Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize intermediate layers\n",
    "print(\"=== CNN Layer Visualization ===\")\n",
    "\n",
    "# Create a model that outputs intermediate layers\n",
    "layer_outputs = [layer.output for layer in cnn_model.layers if 'conv' in layer.name or 'pool' in layer.name]\n",
    "activation_model = keras.models.Model(inputs=cnn_model.input, outputs=layer_outputs)\n",
    "\n",
    "# Get activations for a test sample\n",
    "sample_idx = 0\n",
    "sample = X_test_cnn[sample_idx:sample_idx+1]\n",
    "activations = activation_model.predict(sample)\n",
    "\n",
    "# Visualize first convolutional layer activations\n",
    "first_conv_layer = activations[0]  # First conv layer output\n",
    "print(f\"First conv layer activation shape: {first_conv_layer.shape}\")\n",
    "\n",
    "# Plot some filters\n",
    "n_filters_to_plot = min(16, first_conv_layer.shape[-1])\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i in range(n_filters_to_plot):\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    plt.imshow(first_conv_layer[0, 0, :, i], cmap='viridis')\n",
    "    plt.title(f'Filter {i+1}')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.suptitle('First Convolutional Layer Activations', fontsize=16, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Model Comparison and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with a simple Dense model\n",
    "print(\"=== Model Comparison: Dense Network ===\")\n",
    "\n",
    "# Create a simple Dense model\n",
    "dense_model = keras.Sequential([\n",
    "    layers.Input(shape=(X_train_scaled.shape[1],)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "dense_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the Dense model\n",
    "dense_history = dense_model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=0,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Evaluate both models\n",
    "cnn_test_loss, cnn_test_acc = cnn_model.evaluate(X_test_cnn, y_test, verbose=0)\n",
    "dense_test_loss, dense_test_acc = dense_model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "\n",
    "print(f\"CNN Model - Test Accuracy: {cnn_test_acc:.4f}, Test Loss: {cnn_test_loss:.4f}\")\n",
    "print(f\"Dense Model - Test Accuracy: {dense_test_acc:.4f}, Test Loss: {dense_test_loss:.4f}\")\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['val_accuracy'], label='CNN')\n",
    "plt.plot(dense_history.history['val_accuracy'], label='Dense')\n",
    "plt.title('Validation Accuracy Comparison')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(['CNN', 'Dense'], [cnn_test_acc, dense_test_acc])\n",
    "plt.title('Test Accuracy Comparison')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Conclusions and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and insights\n",
    "print(\"=== EDA and CNN Implementation Summary ===\")\n",
    "print(\"\\nKey Insights from EDA:\")\n",
    "print(\"1. Dataset Overview:\")\n",
    "print(f\"   - Total samples: {len(df)}\")\n",
    "print(f\"   - Number of features: {len(df.columns)}\")\n",
    "print(f\"   - Time period: {df['Year'].min()} to {df['Year'].max()}\" if 'Year' in df.columns else \"   - No time period data available\")\n",
    "print(\"\\n2. Data Quality:\")\n",
    "print(f\"   - Missing values: {df.isnull().sum().sum()}\")\n",
    "print(f\"   - Duplicate rows: {df.duplicated().sum()}\")\n",
    "print(\"\\n3. Key Findings:\")\n",
    "\n",
    "# Calculate some key statistics\n",
    "if 'CO2_Emissions' in df.columns:\n",
    "    avg_co2 = df['CO2_Emissions'].mean()\n",
    "    max_co2 = df['CO2_Emissions'].max()\n",
    "    min_co2 = df['CO2_Emissions'].min()\n",
    "    print(f\"   - Average CO2 emissions: {avg_co2:.2f}\")\n",
    "    print(f\"   - CO2 emissions range: {min_co2:.2f} - {max_co2:.2f}\")\n",
    "\n",
    "if 'Efficiency_Rate' in df.columns:\n",
    "    avg_efficiency = df['Efficiency_Rate'].mean()\n",
    "    print(f\"   - Average efficiency rate: {avg_efficiency:.2%}\")\n",
    "\n",
    "print(f\"\\n4. Model Performance:\")\n",
    "print(f\"   - CNN Test Accuracy: {cnn_test_acc:.4f}\")\n",
    "print(f\"   - Dense Network Test Accuracy: {dense_test_acc:.4f}\")\n",
    "print(f\"   - Performance difference: {abs(cnn_test_acc - dense_test_acc):.4f}\")\n",
    "\n",
    "if abs(cnn_test_acc - dense_test_acc) < 0.05:\n",
    "    print(\"   - Note: Both models performed similarly, suggesting the problem might\")\n",
    "    print(\"     be better suited for traditional machine learning approaches.\")\n",
    "else:\n",
    "    better_model = \"CNN\" if cnn_test_acc > dense_test_acc else \"Dense Network\"\n",
    "    print(f\"   - The {better_model} performed better for this classification task.\")\n",
    "\n",
    "print(\"\\n5. Recommendations:\")\n",
    "print(\"   - Consider feature engineering to improve model performance\")\n",
    "print(\"   - Experiment with different CNN architectures (deeper/wider)\")\n",
    "print(\"   - Try different activation functions and optimizers\")\n",
    "print(\"   - Implement cross-validation for more robust evaluation\")\n",
    "print(\"   - Consider ensemble methods to combine multiple models\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}